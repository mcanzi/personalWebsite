<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>information flow order in speech perception</title>
        <style>

    html body {
        font-family: 'Mina', sans-serif;
        background-color: #F6FEFC;
    }

    :root {
        --accent: #195E4D;
        --border-width:  0 ;
    }

</style>


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mina">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=VT323">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/lakeside-light.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="/css/main.css">




 


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
    

    <script>hljs.initHighlightingOnLoad();</script>







<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link href="https://gitcdn.github.io/bootstrap-toggle/2.2.2/css/bootstrap-toggle.min.css" rel="stylesheet">
<script src="https://gitcdn.github.io/bootstrap-toggle/2.2.2/js/bootstrap-toggle.min.js"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>


<script>
$(document).ready(function(){
    
  var input = $('#night-mode-toggle');
  var container = $('#bigbody');
  var stat = $('#button-status');
  
  container.toggleClass(localStorage.toggled);
  stat.bootstrapToggle(localStorage.button).change();
  
  input.on('click', function() {
      if (localStorage.toggled != "-nightmode" ) {
          container.toggleClass("-nightmode", true );
          localStorage.toggled = "-nightmode";
          localStorage.button = "on";
       } else {
          container.toggleClass("-nightmode", false );
          localStorage.toggled = "";
          localStorage.button = "off"
       }
  })
});
</script>
 <meta name="generator" content="Hugo 0.48" />
        
        

    
    <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon/favicon-16x16.png">
    <link rel="manifest" href="/img/favicon/site.webmanifest">
    <link rel="mask-icon" href="/img/favicon/safari-pinned-tab.svg" color="#000000">
    <link rel="shortcut icon" href="/img/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#2b5797">
    <meta name="msapplication-config" content="/img/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    
    
    
    <meta property="og:title" content="information flow order in speech perception">
    <meta property="og:type" content="article">
      
      <meta name="twitter:card" content="summary">
      <meta name="twitter:image" content="//favicon/image-1.png" >
      
    <meta property="description" content="PhD project">
    <meta property="og:description" content="PhD project">
    
    <meta name="twitter:creator" content="">
    <meta name="twitter:site" content="">
    
    </head>

    
    
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <body id = "bigbody">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand visible-xs" href="#">information flow order in speech perception</a>
                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="collapse navbar-collapse">
                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">home</a></li>
                            
                                <li><a href="/post/">about me</a></li>
                            
                                <li><a href="/project/">selected presentations and research</a></li>
                            
                                <li><a href="/tags/">tags</a></li>
                            
                        </ul>
                    
                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:massimiliano.canzi@postgrad.manchester.ac.uk"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/mcanzi"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://twitter.com/mc_anzi/"><i class="fa fa-twitter"></i></a></li>
                            
                            <li id="night-mode-toggle">
    <input type="checkbox" id = "button-status"
        data-toggle="toggle"
        data-width = "10"
        data-height = "1"
        data-on="<i class='fa fa-moon-o fa-lg' style='vertical-align:25%'></i>"
        data-off= "<i class='fa fa-sun-o fa-lg' style='vertical-align:25%'></i>"
        data-style="ios"
        data-onstyle = "default">
</li>
                        </ul>
                    
                </div>
            </div>
        </nav>


<main>

    <div class="item">

    
    
    

    
      
      
    

    <h4><a href="/project/erp/">information flow order in speech perception</a></h4>
    <h5>September 22, 2017 - 10 minutes</h5>
    <h5>PhD project</h5>

    
    
    <a href="/tags/erp">
        <kbd class="item-tag"> erp </kbd>
    </a>
    
    <a href="/tags/perception">
        <kbd class="item-tag"> perception </kbd>
    </a>
    

</div>


    <br> <div class="text-justify">

<h2 id="overview">Overview</h2>

<p><strong>Aims and Objectives</strong></p>

<p>• To explore how different levels of speech processing interact with each other to facilitate the understanding of speech.</p>

<p>• To test experimentally the relative timing of different levels in speech processing.</p>

<p>• To collect EEG (electroencephalography) data from a relevant sample size on brain re- sponses to auditory stimuli.</p>

<p><strong>Background:</strong> Two main currents of thought in psycholinguistics have categorised speech per- ception to be either unidirectional or interactive. In a unidirectional model of speech perception (e.g. McQueen et al. 2009), components of speech are processed by the brain in a tidy chain of processes, happening one after the other, ordered hierarchically from the phonetic to the syntactic level. In the interactive model(s), such as the TRACE model (McClelland and Elman 1986), speech is analysed in the brain as a series of processes interacting with one another at the same time. This means that a more abstract level of speech, such as lexical activity, could influence our perception af a much less abstract tier, such as phonetic recognition. The Ganong effect is a typical example of interaction between lexical knowledge and phoneme recognition (Ganong 1980).</p>

<p>Ganong (1980) found out that ambiguous phonemes varying in Voice Onset Time (e.g. /d/ and /t/) are resolved more often in a way in which a word is obtained rather than a non-word. A /d/-/t/ continuum is more likely to be resolved as /d/ in (d-t)ash but as /t/ in (d-t)ask, as tash and dask are non-words, while task and dash are acceptable English words. The effect of lexical knowledge was stronger at the phoneme boundary - when it wass less clear which of the two sounds had been produced - rather than at the two ends of the continuum. For this reason, Ganong (1980) concludes that the effect of lexical knowledge arises at stages “sensitive to both lexical knowledge and auditory information”. Another known lexical effect is the restoration effect (Warren 1970). When a non-speech sound (such as a cough or white noise) completely replaces a speech sound in a sentence, listeners believe they have heard the missing sound. The influence of lexical knowledge allows a missing cue to a word to be mediated by all other cues (i.e. other phonemes) in the recognition of that word.</p>

<p>The debate outlined above in the area of speech perception is a specific instance of a wider current linguistic debate on whether the architecture of grammar is strictly serial and hierar- chical (e.g. Bermúdez-Otero 2012), or not (e.g. Johnson 2006). The strong link between the architecture of perception and grammar makes the findings of this research project extremely relevant to a much wider pool of researchers in linguistics than just to those who are directly interested in speech perception, alone.</p>

<p><strong>Research questions:</strong> The underlying question is to try and understand, as anticipated in the background section of this proposal, whether speech perception presents an interactive or feed-forward, hierarchical structure. By studying the direct responses of the brain (i.e. brain-related event potentials or ERP) to auditory stimuli, different experiments will try and determine the nature (i.e. phonological vs. lexical) of those responses, in order to establish a tidy time-line of events in the brain, if there is one, and the interaction of those events, otherwise. Research questions could then be summarised as:</p>

<p>• When in the process of speech perception do we note activation of phonological and lexical processing by the brain?</p>

<p>• Can lexical and phonetic/phonological levels interact directly in speech perception?</p>

<p>• Is speech perception hierarchically organised or interactive?</p>

<p><strong>Methods:</strong> Event-related brain potentials (ERP) are components of brain activity used to study complex cognitive processes with very detailed time resolution. The use of electroen- cephalography for the collection of ERP data (i.e. electric potential shifts in brain waves) is a non-invasive method for the study of brain-waves and it allows researchers in fields such as cognitive sciences and linguistics to study the brain’s direct reaction to stimuli of different na- ture, including auditory and visual. This kind of experimental methodology has been part of linguistic research since the ’80s with work such as that of Kutas and Hillyard (1980). Because the goal of this work is to see whether there is interaction between processes (or feedback, even) it is of utmost importance to be able to use techniques that offer a very high time resolution, so that we can identify with certainty when certain processes and responses to stimuli are found, in opposition to ‘where’, which could be answered best with other techniques, such as functional Magnetic Resonance Imaging (fMRI). I will use three established ERP experimental paradigms to tackle different angles of speech perception and to determine the nature and function of early responses of the brain to auditory stimuli. I will build on previous research in this area (e.g. Praamstra et al. 1994) in order to illuminate precise timing of lexical and phonetic processing in a hitherto under-researched context.</p>

<p><strong>ERP components:</strong> The N400 ERP component - a shift in electrical potential found in the brain as a response to a stimulus - occurs approximately 220 to 660 ms post word-onset and is distributed in the area of the scalp with a medial centro-parietal focus. A series of past studies (e.g. Kutas and Hillyard 1980) have shown evidence that the amplitude of N400 is negatively correlated with the probability of a word occurring in the sentence context provided by a certain stimulus (i.e., a sentence like: “The piano is out of pizza” would probably elicit a N400 response broadly 250 - 600 ms post word-onset). One of the distinguishing features of the N400 ERP component is that it is sensitive to semantic manipulations, while being fairly non-responsive to other linguistic changes (e.g., syntactic or phonetic). The N400 must then be linked to some sort of semantic memory retrieval or the integration of semantic elements with already existing context (Hagoort et al. 2004).</p>

<p>There’s a number of pre-N400 effects that have been observed in relation to sentence compre- hension and it is still unclear whether they are different from the N400 component or whether they are just variations. The PMN (Praamstra et al. 1994) is one of those effects, usually found 150 - 350 ms post word-onset. The PMN (Phonological Mismatch Negativity) is thought to react negatively to lower probability phonemes rather than to higher-probability ones (Groppe et al. 2010). However, we are still not sure whether this pre-N400 response is simply a response to phonological mismatch or whether lexical information is also considered. Understanding the nature of this early responses of the brain to auditory stimuli is essential in the understanding of the overall structure of speech perception.</p>

<p><strong>Hypotheses:</strong></p>

<p>• <strong>H1: Lexical knowledge and activity trigger an early (ca. 200 ms) response of the brain to auditory stimuli.</strong> By studying the nature of early responses of the brain to the presentation of auditory stimuli including both words and nonce words, it will be possible to obtain a better understanding of the nature of what levels are processed and when. Understanding if responses found earlier than 400 ms (i.e. N400) are to phoneme probability only - rather than to word meaning and context - can, in future experiments, lead to a better understanding of the structure of perception of speech. The hypothesis relies on the idea that the interaction between lexical activation and phonetic recognition is possible, as Ganong (1980) and Warren (1970) have shown, although it is still unclear what the time-line of events is.
• <strong>H2: In case H1 is proved to be true: early responses of the brain to the presenta- tion of auditory stimuli are simultaneously phonological and lexical, with no particular time-constrained order</strong> (i.e. if lexical responses of the brain are found as early as 200 ms post stimulus onset, phonological ones are not necessarily found at earlier stages).</p>

<p><strong>Research conducted so far:</strong> In the first few months of my PhD, I have created and refined the methodology for a pilot study. The same method will be used for the core research in this project. The pilot study uses a very similar methodology to that of Astheimer and Sanders (2011), although presenting a very different research focus to that of the original study. The focus of the first experiment is to evaluate H1 and to present a solid foundation for future work on the matter of interaction of early responses of speech perception.
The experiment tests the responses of the brain when participants are presented with nonce words or real words, in separate contexts. Because nonce words are void of lexical meaning, no early lexical responses should be found. However, because of the unclear nature of these early components, it is still unclear whether some response by the brain should or should not be present. The hypothesis of this experiment is that if early responses are found while testing listeners with nonce word stimuli, all responses must be phonological (i.e. sound mismatch) as no lexical component is present.</p>

<p><strong>Methodology summary:</strong> The following summary refers to the methodology of the pilot study, which will test what responses are given by the brain when there is a phonological mismatch (i.e. the sound heard is not the sound that was expected) in the presentation of nonce words. The following account aims at getting the reader to understand the overall methodology of the pilot study while keeping the paragraph brief and accessible to read.</p>

<p>Participants are trained to hear and remember three pairs of nonce words. The nonce words are bidupu, bibapu, tapabi, piputu, dubapu, dipida. Each pair is presented so that the order of the two items is always the same (i.e. in the bidupu-tapabi pair, tapabi is always heard second). After the participants have mastered recognising all of the stimuli, EEG data is collected while presenting the auditory stimuli - in pairs - back to them, with the difference that some of the pairs will present a different syllable onset for the second word in the pair (e.g. bidupu-tapabi might be presented as bidupu-bapabi). This mismatch in sound perception should elicit early responses of the brain thought to be connected with sound-mismatch. However, no lexical- mismatch response should be found because no lexical activation is present due to the use of nonce words. The experiment is then repeated using three pairs of real words. The goal of the experiment is to try and elicit early responses and try and understand their function.</p>

<p><strong>Future experiments:</strong> After determining the function of early responses to auditory stimuli by the brain, future experiments will deal with the evaluation of H2 and the interaction between levels of speech processing. The introduction of the Ganong effect (1980) in the collection of EEG data is a novel idea for the study of word disambiguation in perception. By analysing when certain brain responses are found - and by knowing what those particular responses refer to thanks to the first experiment of this research project - it will be possible to determine whether semantic processing appears to interact and mediate phoneme recognition. The results of the first experiment - and its pilot - will determine the course of action for future experiments. The designs will be, in fact, influenced by how responses of the brain to auditory stimuli are categorised in the first phase of this research.</p>

<p><strong>Summary of achievements so far:</strong></p>

<p>• Analysis of background research and narrowing of research question.</p>

<p>• Formulation of hypotheses and planning of experiments.</p>

<p>• Creation of a detailed methodology and stimuli for the realisation of the pilot study.</p>

<p>• Training in collection and analysis of EEG data.</p>
</div>

    
    

    

        <h4 class="page-header">Related</h4>

         <div class="item">

    
    
    

    
    
      
    

    <h4><a href="/2018/09/17/skills/">skills</a></h4>
    <h5>September 17, 2018 - 1 minutes</h5>
    <h5></h5>

    
    
    <a href="/tags/r">
        <kbd class="item-tag"> r </kbd>
    </a>
    
    <a href="/tags/praat">
        <kbd class="item-tag"> praat </kbd>
    </a>
    
    <a href="/tags/matlab">
        <kbd class="item-tag"> matlab </kbd>
    </a>
    
    <a href="/tags/erp">
        <kbd class="item-tag"> erp </kbd>
    </a>
    

</div>
 

    

    

</main>

        <footer id = "bigfooter">
            <div style = "padding:15px;">
                <p>Powered by <a href="https://gohugo.io">Hugo</a>. Themed by <a href="https://github.com/nathancday/min_night">min_night</a>.
                </p>
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/"
                title="Creative Commons Attribution 4.0 International license">
                <i class="fa fa-creative-commons" aria-hidden="true"></i> Attribution 4.0 International license
                </a>
            </div>
        </footer>
        
        <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());
          gtag('config', '');
        </script>
       
    </body>

</html>

