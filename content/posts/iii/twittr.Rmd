---
title: "Twitter data and sentiment scores in R and Python"
author: "Massimiliano Canzi"
date: 2021-01-22
tags: ["Python", "R", "ggplot"]
draft: false
---

# Getting started

_Turn on dark mode in the right hand corner of this website for the best experience_ 

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The aim of this brief post is to present an introduction to the collection and analysis of Twitter data with `R` and `Python`. Tweets are extracted in `R`. Sentiment analysis is carried out in `Python` with `nltk` and `VADER`. The sentiment scores, combined with the previously acquired twitter data, aree then brought back into `R` for a visual exploration using `ggplot()` and `tidyverse`. 
First, we load the necessary libraries

## Packages

```{r, message = FALSE, warning = FALSE}
library(twitteR)
library(ROAuth)
library(tidyverse)
library(viridis)
library(ggdark)
library(reticulate)
library(lme4)
```

In order to be able to download your own tweet data, you need __API consumer keys__. You can obtain the keys by creating a __Twitter App__ on the _Developer page_. This tutorial does not cover how this process is carried out. Many tutorials have been made available on the topic and can they can be easily found on the internet. One example is [here](https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html)

The code chunk below loads the saved authorization details and sets up my app. The first time you set up your app, you will need to provide the keys and then save your `authentication.Rdata` file to disk as highlighted in the tutorial abovee.

```{r, eval=FALSE}
load('twitter authentication.Rdata')

setup_twitter_oauth(credentials$consumerKey, credentials$consumerSecret, credentials$oauthKey, credentials$oauthSecret)
```

Now that we have created a direct link to Twitter, we can download tweet data. Below, I downloaded the latest 3000 tweets by __Joe Biden__, excluding re-tweets and replies to other people's tweets. I saved the downloaded data to a data frame and then to a `.csv` file.

```{r, eval=FALSE}
jb <- twListToDF(userTimeline("JoeBiden", n = 3000, includeRts = FALSE, excludeReplies = TRUE)) 

write_csv(jb, "biden_tweets.csv")
```

In Python, load the required libraries for sentiment analysis with `VADER` and download the `VADER` lexicon (you only need to download the lexicon once). 

```{python, eval=FALSE}
import nltk
import pandas as pd
import numpy as np

#  nltk.download('vader_lexicon')
```

Now it's time to calculate sentiment scores for each tweet and compound values (from `-1` to `+1`, where `-1` represents a very negative sentiment score and `+1` a very positive one). We also apply a `comp_score` label `"pos"` for positive tweets (`compound > 0`) and a `"neg"` label for negative (`compound < 0`). 

With `reticulate`, the `Python` code should work from an `RStudio` environment. However, if it doesn't work straight away, you should still be able to run this natively in `Python`.

```{python, eval = FALSE}
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

df = pd.read_csv("biden_tweets.csv")
df['scores'] = df['text'].apply(lambda review: sid.polarity_scores(review))
df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])
df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')

df.to_csv("bidne_vader.csv")
```

We can now load the data frame with sentiment scores back into `R`.

```{r, message = FALSE, warning = FALSE}
df <- read_csv("biden_tweets_vader.csv")
```

The following chunk of code creates two scatter plots in `ggplot()` __Figure 1__ displays compound sentiment scores ~ number of re-tweets, with number of favorites on the z axis. __Figure 2__ swaps the number of favorites and number of re-tweets variables.

Overall, from a quick visual exploration, there does not appear to be any visible correlation between the number of re-tweets and / or favorites and how positive or negative a certain tweet is. However, number of re-tweets and favorites seem to be correlated, which is hardly a surprise. 

```{r, warning = FALSE, message = FALSE}
filter(df, df$compound != 0) %>%
  filter(retweetCount <= 10000) %>%
  ggplot(aes(x = compound, y = retweetCount, color = favoriteCount)) +
  geom_vline(xintercept = 0, size = 0.5, alpha = 0.8, linetype = "dashed" ) +
  geom_point(size = 0.8, alpha = 0.8) +
  ylim(0, 10000) +
  xlab("Sentiment Score") +
  ylab("Retweet Count") +
  scale_color_viridis(option = "viridis", name = "Favorite Count", limits = c(0, 30000)) +
  ggtitle(paste("Retweet count by VADER sentiment scores for", length(df$text), "tweets by Joe Biden.")) +
  dark_theme_minimal()
```

```{r, messagee = FALSE}
filter(df, df$compound != 0) %>%
  filter(favoriteCount <= 30000) %>%
  ggplot(aes(x = compound, y = favoriteCount, color = retweetCount)) +
  geom_vline(xintercept = 0, size = 0.5, alpha = 0.8, linetype = "dashed" ) +
  geom_point(size = 0.8, alpha = 0.8) +
  ylim(0, 30000) +
  xlab("Sentiment Score") +
  ylab("Favorite Count") +
  scale_color_viridis(option = "plasma", name = "Retweet Count", limits = c(0, 10000)) +
  ggtitle(paste("Favorite count by VADER sentiment scores for", length(df$text), "tweets by Joe Biden.")) +
  dark_theme_minimal()
```

__Figures 3__ and __4__ present tweet sentiment score values by hour of day and month of year. The z dimension is coded to the number of favorites for each tweet.

```{r}
df$createdHour <- strftime(df$created, format="%H:%M:%S")
df$createdHour <- as.POSIXct(df$createdHour, format="%H:%M:%S")

filter(df, df$compound != 0) %>%
  filter(favoriteCount <= 30000) %>%
  ggplot(aes(x = createdHour, y = compound, color = favoriteCount)) +
  scale_x_datetime(date_labels = "%H:%M", date_breaks = "4 hours") +
  geom_hline(yintercept = 0, size = 0.5, alpha = 0.8, linetype = "dashed" ) +
  geom_point(size = 1.5, alpha = 0.8) +
  xlab("Time of Day") +
  ylab("Sentiment Score") +
  scale_color_viridis(option = "magma", name = "Favorite Count", limits = c(0, 30000)) +
  ggtitle(paste("Sentiment scores by time of day for", length(df$text), "tweets by Joe Biden.")) +
  dark_theme_minimal()
```

```{r}
filter(df, df$compound != 0) %>%
  filter(favoriteCount <= 30000) %>%
  ggplot(aes(x = created, y = compound, color = favoriteCount)) +
  scale_x_datetime(date_labels = "%b", date_breaks = "1 month") +
  geom_hline(yintercept = 0, size = 0.5, alpha = 0.8, linetype = "dashed" ) +
  geom_point(size = 1.5, alpha = 0.8) +
  xlab("Month") +
  ylab("Sentiment Score") +
  scale_color_viridis(option = "magma", name = "Favorite Count", limits = c(0, 30000)) +
  ggtitle(paste("Sentiment scores by month for", length(df$text), "tweets by Joe Biden.")) +
  dark_theme_minimal()
```

Here we load a similar data frame containing 10 years of tweets by Donald Trump. The data are also readily available online and were downloaded from Twitter with the method highlighted above. For this data frame, I also calculated sentiment scores and labels using `Vader` in `Python`. 

```{r, warning=FALSE}
dt <- read_csv("realdonaldtrump_vader.csv")
```

Re-tweets and favorites over time, coloured by compound score labels (i.e. `"negative"` vs `"positive"`)

```{r}
dt$date <- as.POSIXct(dt$date, format="%Y/%m/%d %H:%M:%S")
y_max <- median(dt$retweets) + 8 * sd(dt$retweets)
fav_max <- median(dt$favorites) + 8 * sd(dt$favorites)
dt$year <- as.factor(as.POSIXlt(dt$date)$year + 1900)

dt2 <- filter(dt, compound != 0) %>%
  filter(year %in% c("2016", "2017", "2018", "2019", "2020")) %>%
  filter(favorites < fav_max) %>%
  filter(retweets < y_max)

ggplot(dt2, aes(x = date, y = retweets, color = comp_score)) +
  scale_x_datetime(date_labels = "%y", date_breaks = "1 year") +
  geom_point(size = 0.5, alpha = 0.8) +
  ylim(0, y_max) +
  xlab("Year") +
  ylab("Retweets") +
  scale_color_manual(values = c("indianred2", "deepskyblue4"), name = "Sentiment Label", labels = c("Negative", "Positive")) +
  ggtitle(paste("Retweet scores by time and SL for", length(dt2$content), "tweets by @realDonaldTrump")) +
  #facet_grid(facets = dt2$comp_score) +
  dark_theme_minimal()
```

```{r}
ggplot(dt2, aes(x = date, y = favorites, color = comp_score)) +
  scale_x_datetime(date_labels = "%y", date_breaks = "1 year") +
  geom_point(size = 0.5, alpha = 0.8) +
  ylim(0, fav_max) +
  xlab("Year") +
  ylab("Favorites") +
  scale_color_manual(values = c("indianred2", "deepskyblue4"), name = "Sentiment Label", labels = c("Negative", "Positive")) +
  ggtitle(paste("Retweet scores by time and SL for", length(dt$content), "tweets by @realDonaldTrump")) +
  dark_theme_minimal()
```

```{r}
ggplot(dt2, aes(x = compound, y = retweets, color = year)) +
  geom_point(size = 0.5, alpha = 0.8) +
  xlim(-1, 1) +
  ylim(0, y_max) +
  xlab("Sentiment") +
  ylab("Retweet") +
  scale_color_viridis_d(option = "plasma", name = "Year") +
  ggtitle(paste("Retweets by sentiment scores over", length(dt$content), "tweets by @realDonaldTrump")) +
  dark_theme_minimal()
```

We have seen a few examples of how too tackle the investigation of the data distribution moree generally. However, we can also get specific and look for the mention of specific words or phrases.

We could, for example, isolate tweets that contain _fake news_ using `grepl` and regular expressions. We can plot their re-tweet count over time. We can also map tweets that contain _CNN_ to the same plot and see what the overlap is between _CNN_ and _fake news_. 

```{r}
dt_fn <- filter(dt2, grepl('fake news|FAKE NEWS', content))
dt_cnn <- filter(dt2, grepl('CNN|cnn', content))

ggplot(dt_fn, aes(x = date, y = retweets, color = )) +
  scale_x_datetime(date_labels = "%y", date_breaks = "1 year") +
  geom_point(size = 1.2, alpha = 0.8, aes(color = "red")) +
  geom_point(data = dt_cnn, size = 0.8, alpha = 0.6, aes(color = "blue")) +
  ylim(0, y_max) +
  xlab("Year") +
  ylab("Retweets") +
  scale_color_discrete(name = "Expression", labels = c("fake news", "CNN")) +
  ggtitle(paste("\"fake news\" and CNN over time in", length(dt$content), "tweets by @realDonaldTrump")) +
  dark_theme_minimal()
```
We can also use our data to run statistical tests. There's currently no hypothesis that I'm aiming to test, as most of the code from this short tutorial should be mainly considered an exercise in data visualisation. However.. 

```{r}
summary(lm(compound ~ year, data = dt2))
```

