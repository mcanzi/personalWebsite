---
title: "Twitter data and sentiment scores in R and Python"
author: "Massimiliano Canzi"
date: 2021-01-22
tags: ["Python", "R", "ggplot"]
draft: false
---



<div id="getting-started" class="section level1">
<h1>Getting started</h1>
<p><em>Turn on dark mode in the right hand corner of this website for the best experience</em></p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The aim of this brief post is to present an introduction to the collection and analysis of Twitter data with <code>R</code> and <code>Python</code>. Tweets are extracted in <code>R</code>. Sentiment analysis is carried out in <code>Python</code> with <code>nltk</code> and <code>VADER</code>. The sentiment scores, combined with the previously acquired twitter data, aree then brought back into <code>R</code> for a visual exploration using <code>ggplot()</code> and <code>tidyverse</code>.
First, we load the necessary libraries</p>
<div id="packages" class="section level2">
<h2>Packages</h2>
<pre class="r"><code>library(twitteR)
library(ROAuth)
library(tidyverse)
library(viridis)
library(ggdark)
library(reticulate)
library(lme4)</code></pre>
<p>In order to be able to download your own tweet data, you need <strong>API consumer keys</strong>. You can obtain the keys by creating a <strong>Twitter App</strong> on the <em>Developer page</em>. This tutorial does not cover how this process is carried out. Many tutorials have been made available on the topic and can they can be easily found on the internet. One example is <a href="https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html">here</a></p>
<p>The code chunk below loads the saved authorization details and sets up my app. The first time you set up your app, you will need to provide the keys and then save your <code>authentication.Rdata</code> file to disk as highlighted in the tutorial abovee.</p>
<pre class="r"><code>load(&#39;twitter authentication.Rdata&#39;)

setup_twitter_oauth(credentials$consumerKey, credentials$consumerSecret, credentials$oauthKey, credentials$oauthSecret)</code></pre>
<p>Now that we have created a direct link to Twitter, we can download tweet data. Below, I downloaded the latest 3000 tweets by <strong>Joe Biden</strong>, excluding re-tweets and replies to other people’s tweets. I saved the downloaded data to a data frame and then to a <code>.csv</code> file.</p>
<pre class="r"><code>jb &lt;- twListToDF(userTimeline(&quot;JoeBiden&quot;, n = 3000, includeRts = FALSE, excludeReplies = TRUE)) 

write_csv(jb, &quot;biden_tweets.csv&quot;)</code></pre>
<p>In Python, load the required libraries for sentiment analysis with <code>VADER</code> and download the <code>VADER</code> lexicon (you only need to download the lexicon once).</p>
<pre class="python"><code>import nltk
import pandas as pd
import numpy as np

#  nltk.download(&#39;vader_lexicon&#39;)</code></pre>
<p>Now it’s time to calculate sentiment scores for each tweet and compound values (from <code>-1</code> to <code>+1</code>, where <code>-1</code> represents a very negative sentiment score and <code>+1</code> a very positive one). We also apply a <code>comp_score</code> label <code>"pos"</code> for positive tweets (<code>compound &gt; 0</code>) and a <code>"neg"</code> label for negative (<code>compound &lt; 0</code>).</p>
<p>With <code>reticulate</code>, the <code>Python</code> code should work from an <code>RStudio</code> environment. However, if it doesn’t work straight away, you should still be able to run this natively in <code>Python</code>.</p>
<pre class="python"><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()

df = pd.read_csv(&quot;biden_tweets.csv&quot;)
df[&#39;scores&#39;] = df[&#39;text&#39;].apply(lambda review: sid.polarity_scores(review))
df[&#39;compound&#39;]  = df[&#39;scores&#39;].apply(lambda score_dict: score_dict[&#39;compound&#39;])
df[&#39;comp_score&#39;] = df[&#39;compound&#39;].apply(lambda c: &#39;pos&#39; if c &gt;=0 else &#39;neg&#39;)

df.to_csv(&quot;bidne_vader.csv&quot;)</code></pre>
<p>We can now load the data frame with sentiment scores back into <code>R</code>.</p>
<pre class="r"><code>df &lt;- read_csv(&quot;biden_tweets_vader.csv&quot;)</code></pre>
<p>The following chunk of code creates two scatter plots in <code>ggplot()</code> <strong>Figure 1</strong> displays compound sentiment scores ~ number of re-tweets, with number of favorites on the z axis. <strong>Figure 2</strong> swaps the number of favorites and number of re-tweets variables.</p>
<p>Overall, from a quick visual exploration, there does not appear to be any visible correlation between the number of re-tweets and / or favorites and how positive or negative a certain tweet is. However, number of re-tweets and favorites seem to be correlated, which is hardly a surprise.</p>
<pre class="r"><code>filter(df, df$compound != 0) %&gt;%
  filter(retweetCount &lt;= 10000) %&gt;%
  ggplot(aes(x = compound, y = retweetCount, color = favoriteCount)) +
  geom_vline(xintercept = 0, size = 0.5, alpha = 0.8, linetype = &quot;dashed&quot; ) +
  geom_point(size = 0.8, alpha = 0.8) +
  ylim(0, 10000) +
  xlab(&quot;Sentiment Score&quot;) +
  ylab(&quot;Retweet Count&quot;) +
  scale_color_viridis(option = &quot;viridis&quot;, name = &quot;Favorite Count&quot;, limits = c(0, 30000)) +
  ggtitle(paste(&quot;Retweet count by VADER sentiment scores for&quot;, length(df$text), &quot;tweets by Joe Biden.&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>filter(df, df$compound != 0) %&gt;%
  filter(favoriteCount &lt;= 30000) %&gt;%
  ggplot(aes(x = compound, y = favoriteCount, color = retweetCount)) +
  geom_vline(xintercept = 0, size = 0.5, alpha = 0.8, linetype = &quot;dashed&quot; ) +
  geom_point(size = 0.8, alpha = 0.8) +
  ylim(0, 30000) +
  xlab(&quot;Sentiment Score&quot;) +
  ylab(&quot;Favorite Count&quot;) +
  scale_color_viridis(option = &quot;plasma&quot;, name = &quot;Retweet Count&quot;, limits = c(0, 10000)) +
  ggtitle(paste(&quot;Favorite count by VADER sentiment scores for&quot;, length(df$text), &quot;tweets by Joe Biden.&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><strong>Figures 3</strong> and <strong>4</strong> present tweet sentiment score values by hour of day and month of year. The z dimension is coded to the number of favorites for each tweet.</p>
<pre class="r"><code>df$createdHour &lt;- strftime(df$created, format=&quot;%H:%M:%S&quot;)
df$createdHour &lt;- as.POSIXct(df$createdHour, format=&quot;%H:%M:%S&quot;)

filter(df, df$compound != 0) %&gt;%
  filter(favoriteCount &lt;= 30000) %&gt;%
  ggplot(aes(x = createdHour, y = compound, color = favoriteCount)) +
  scale_x_datetime(date_labels = &quot;%H:%M&quot;, date_breaks = &quot;4 hours&quot;) +
  geom_hline(yintercept = 0, size = 0.5, alpha = 0.8, linetype = &quot;dashed&quot; ) +
  geom_point(size = 1.5, alpha = 0.8) +
  xlab(&quot;Time of Day&quot;) +
  ylab(&quot;Sentiment Score&quot;) +
  scale_color_viridis(option = &quot;magma&quot;, name = &quot;Favorite Count&quot;, limits = c(0, 30000)) +
  ggtitle(paste(&quot;Sentiment scores by time of day for&quot;, length(df$text), &quot;tweets by Joe Biden.&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>filter(df, df$compound != 0) %&gt;%
  filter(favoriteCount &lt;= 30000) %&gt;%
  ggplot(aes(x = created, y = compound, color = favoriteCount)) +
  scale_x_datetime(date_labels = &quot;%b&quot;, date_breaks = &quot;1 month&quot;) +
  geom_hline(yintercept = 0, size = 0.5, alpha = 0.8, linetype = &quot;dashed&quot; ) +
  geom_point(size = 1.5, alpha = 0.8) +
  xlab(&quot;Month&quot;) +
  ylab(&quot;Sentiment Score&quot;) +
  scale_color_viridis(option = &quot;magma&quot;, name = &quot;Favorite Count&quot;, limits = c(0, 30000)) +
  ggtitle(paste(&quot;Sentiment scores by month for&quot;, length(df$text), &quot;tweets by Joe Biden.&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Here we load a similar data frame containing 10 years of tweets by Donald Trump. The data are also readily available online and were downloaded from Twitter with the method highlighted above. For this data frame, I also calculated sentiment scores and labels using <code>Vader</code> in <code>Python</code>.</p>
<pre class="r"><code>dt &lt;- read_csv(&quot;realdonaldtrump_vader.csv&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_double(),
##   id = col_double(),
##   link = col_character(),
##   content = col_character(),
##   date = col_datetime(format = &quot;&quot;),
##   retweets = col_double(),
##   favorites = col_double(),
##   mentions = col_character(),
##   hashtags = col_character(),
##   scores = col_character(),
##   compound = col_double(),
##   comp_score = col_character()
## )</code></pre>
<p>Re-tweets and favorites over time, coloured by compound score labels (i.e. <code>"negative"</code> vs <code>"positive"</code>)</p>
<pre class="r"><code>dt$date &lt;- as.POSIXct(dt$date, format=&quot;%Y/%m/%d %H:%M:%S&quot;)
y_max &lt;- median(dt$retweets) + 8 * sd(dt$retweets)
fav_max &lt;- median(dt$favorites) + 8 * sd(dt$favorites)
dt$year &lt;- as.factor(as.POSIXlt(dt$date)$year + 1900)

dt2 &lt;- filter(dt, compound != 0) %&gt;%
  filter(year %in% c(&quot;2016&quot;, &quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;)) %&gt;%
  filter(favorites &lt; fav_max) %&gt;%
  filter(retweets &lt; y_max)

ggplot(dt2, aes(x = date, y = retweets, color = comp_score)) +
  scale_x_datetime(date_labels = &quot;%y&quot;, date_breaks = &quot;1 year&quot;) +
  geom_point(size = 0.5, alpha = 0.8) +
  ylim(0, y_max) +
  xlab(&quot;Year&quot;) +
  ylab(&quot;Retweets&quot;) +
  scale_color_manual(values = c(&quot;indianred2&quot;, &quot;deepskyblue4&quot;), name = &quot;Sentiment Label&quot;, labels = c(&quot;Negative&quot;, &quot;Positive&quot;)) +
  ggtitle(paste(&quot;Retweet scores by time and SL for&quot;, length(dt2$content), &quot;tweets by @realDonaldTrump&quot;)) +
  #facet_grid(facets = dt2$comp_score) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>ggplot(dt2, aes(x = date, y = favorites, color = comp_score)) +
  scale_x_datetime(date_labels = &quot;%y&quot;, date_breaks = &quot;1 year&quot;) +
  geom_point(size = 0.5, alpha = 0.8) +
  ylim(0, fav_max) +
  xlab(&quot;Year&quot;) +
  ylab(&quot;Favorites&quot;) +
  scale_color_manual(values = c(&quot;indianred2&quot;, &quot;deepskyblue4&quot;), name = &quot;Sentiment Label&quot;, labels = c(&quot;Negative&quot;, &quot;Positive&quot;)) +
  ggtitle(paste(&quot;Retweet scores by time and SL for&quot;, length(dt$content), &quot;tweets by @realDonaldTrump&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>ggplot(dt2, aes(x = compound, y = retweets, color = year)) +
  geom_point(size = 0.5, alpha = 0.8) +
  xlim(-1, 1) +
  ylim(0, y_max) +
  xlab(&quot;Sentiment&quot;) +
  ylab(&quot;Retweet&quot;) +
  scale_color_viridis_d(option = &quot;plasma&quot;, name = &quot;Year&quot;) +
  ggtitle(paste(&quot;Retweets by sentiment scores over&quot;, length(dt$content), &quot;tweets by @realDonaldTrump&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We have seen a few examples of how too tackle the investigation of the data distribution moree generally. However, we can also get specific and look for the mention of specific words or phrases.</p>
<p>We could, for example, isolate tweets that contain <em>fake news</em> using <code>grepl</code> and regular expressions. We can plot their re-tweet count over time. We can also map tweets that contain <em>CNN</em> to the same plot and see what the overlap is between <em>CNN</em> and <em>fake news</em>.</p>
<pre class="r"><code>dt_fn &lt;- filter(dt2, grepl(&#39;fake news|FAKE NEWS&#39;, content))
dt_cnn &lt;- filter(dt2, grepl(&#39;CNN|cnn&#39;, content))

ggplot(dt_fn, aes(x = date, y = retweets, color = )) +
  scale_x_datetime(date_labels = &quot;%y&quot;, date_breaks = &quot;1 year&quot;) +
  geom_point(size = 1.2, alpha = 0.8, aes(color = &quot;red&quot;)) +
  geom_point(data = dt_cnn, size = 0.8, alpha = 0.6, aes(color = &quot;blue&quot;)) +
  ylim(0, y_max) +
  xlab(&quot;Year&quot;) +
  ylab(&quot;Retweets&quot;) +
  scale_color_discrete(name = &quot;Expression&quot;, labels = c(&quot;fake news&quot;, &quot;CNN&quot;)) +
  ggtitle(paste(&quot;\&quot;fake news\&quot; and CNN over time in&quot;, length(dt$content), &quot;tweets by @realDonaldTrump&quot;)) +
  dark_theme_minimal()</code></pre>
<p><img src="/posts/iii/twittr_files/figure-html/unnamed-chunk-15-1.png" width="672" />
We can also use our data to run statistical tests. There’s currently no hypothesis that I’m aiming to test, as most of the code from this short tutorial should be mainly considered an exercise in data visualisation. However..</p>
<pre class="r"><code>summary(lm(compound ~ year, data = dt2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = compound ~ year, data = dt2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.2469 -0.6406  0.2182  0.5425  0.8174 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.20169    0.01092  18.471  &lt; 2e-16 ***
## year2017     0.02623    0.01793   1.463 0.143422    
## year2018     0.04563    0.01624   2.810 0.004962 ** 
## year2019    -0.03352    0.01484  -2.259 0.023915 *  
## year2020     0.06516    0.01824   3.573 0.000355 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6265 on 13669 degrees of freedom
## Multiple R-squared:  0.003194,   Adjusted R-squared:  0.002902 
## F-statistic: 10.95 on 4 and 13669 DF,  p-value: 7.317e-09</code></pre>
</div>
</div>
